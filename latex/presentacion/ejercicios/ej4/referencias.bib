@book{goodfellow2016deep,
  title={Deep Learning},
  author={Goodfellow, I. and Bengio, Y. and Courville, A.},
  isbn={9780262337373},
  series={Adaptive Computation and Machine Learning series},
  url={https://books.google.com.co/books?id=omivDQAAQBAJ},
  year={2016},
  publisher={MIT Press}
}

@article{PIGA2021415,
title = {Deep learning with transfer functions: new applications in system identification⁎⁎This work was partially supported by the European H2020-CS2 project ADMITTED, Grant agreement no. GA832003.},
journal = {IFAC-PapersOnLine},
volume = {54},
number = {7},
pages = {415-420},
year = {2021},
note = {19th IFAC Symposium on System Identification SYSID 2021},
issn = {2405-8963},
doi = {https://doi.org/10.1016/j.ifacol.2021.08.395},
url = {https://www.sciencedirect.com/science/article/pii/S2405896321011691},
author = {Dario Piga and Marco Forgione and Manas Mejari},
keywords = {System identification, neural networks, quantized data, prediction error methods, deep learning},
abstract = {This paper presents a linear dynamical operator described in terms of a rational transfer function, endowed with a well-defined and efficient back-propagation behavior for automatic derivatives computation. The operator enables end-to-end training of structured networks containing linear transfer functions and other differentiable units by exploiting standard deep learning software. Two relevant applications of the operator in system identification are presented. The first one consists in the integration of prediction error methods in deep learning. The dynamical operator is included as the last layer of a neural network in order to obtain the optimal one-step-ahead prediction error. The second one considers identification of general block-oriented models from quantized data. These block-oriented models are constructed by combining linear dynamical operators with static nonlinearities described as standard feed-forward neural networks. A custom loss function corresponding to the log-likelihood of quantized output observations is defined. For gradient-based optimization, the derivatives of the log-likelihood are computed by applying the back-propagation algorithm through the whole network. Two system identification benchmarks are used to show the effectiveness of the proposed methodologies.}
}

@ARTICLE{9372789,
  author={Zhang, Peiying and Wang, Chao and Jiang, Chunxiao and Han, Zhu},
  journal={IEEE Transactions on Industrial Informatics}, 
  title={Deep Reinforcement Learning Assisted Federated Learning Algorithm for Data Management of IIoT}, 
  year={2021},
  volume={17},
  number={12},
  pages={8475-8484},
  doi={10.1109/TII.2021.3064351}}
